{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF19mGkGfU3X",
        "outputId": "5db9e42c-0545-41b4-fad4-4d1eab0c0466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rMTxZUjVUKS"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import gzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2Ef0IS4Da9Y",
        "outputId": "0138c8ed-a7cd-4ace-97b1-0ab69fb605bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "database = pd.read_csv('train.tsv', sep='\\t', nrows=30000)\n",
        "\n",
        "training_input = database['sentence1']\n",
        "training_output = database['sentence2']\n",
        "\n",
        "PADDING_CHAR_CODE=0\n",
        "START_CHAR_CODE=1\n",
        "\n",
        "DEFAULT_INPUT_LENGTH = 20\n",
        "DEFAULT_OUTPUT_LENGTH = 20\n",
        "\n",
        "print(training_input.shape, training_output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000,) (30000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5YgtSfhJhAN"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Embeddings():\n",
        "    def __init__(self, path, vector_dimension):\n",
        "        self.path = path \n",
        "        self.vector_dimension = vector_dimension\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_coefs(word, *arr): \n",
        "        return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "    def get_embedding_index(self):\n",
        "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore'))\n",
        "        return embeddings_index\n",
        "\n",
        "    def create_embedding_matrix(self, tokenizer, max_features):\n",
        "        model_embed = self.get_embedding_index()\n",
        "\n",
        "        embedding_matrix = np.zeros((max_features + 1, self.vector_dimension))\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index > max_features:\n",
        "                break\n",
        "            else:\n",
        "                try:\n",
        "                    embedding_matrix[index] = model_embed[word]\n",
        "                except:\n",
        "                    continue\n",
        "        return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgYNZSvWN80-"
      },
      "source": [
        "# Preprocess data\n",
        "import re\n",
        "def preprocess_sentence(sentence):\n",
        "    ret = sentence.lower()\n",
        "    ret = ret.strip()\n",
        "    ret = re.sub(\"([?.!,])\", \" \\1 \", ret)\n",
        "    ret = re.sub('[\" \"]+', \" \", ret)\n",
        "    ret = re.sub(\"-\", \" \", ret)\n",
        "    ret = ret.strip()\n",
        "    return ret\n",
        "\n",
        "training_input = list(map(lambda x: preprocess_sentence(x), training_input))\n",
        "training_output = list(map(lambda x: preprocess_sentence(x), training_output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_AwOnslJnbt",
        "outputId": "a067130e-26e6-4b06-bacb-8cda7d9cfdbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sentences to tokens\n",
        "original_tokenizer = Tokenizer(num_words=22000)\n",
        "original_tokenizer.fit_on_texts(training_input)\n",
        "\n",
        "clone_tokenizer = Tokenizer(num_words=22000)\n",
        "clone_tokenizer.fit_on_texts(training_output)\n",
        "\n",
        "original_vocab_size, clone_vocab_size = 22000, 22000#len(original_tokenizer.word_counts), len(clone_tokenizer.word_counts)\n",
        "largest_vocab_size = max([original_vocab_size, clone_vocab_size])\n",
        "print(len(original_tokenizer.word_counts), largest_vocab_size, original_vocab_size, clone_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54015 22000 22000 22000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DINxzNONLb9_"
      },
      "source": [
        "#Word embedding\n",
        "embedding = Embeddings(\n",
        "    path = 'glove.6B.200d.txt',\n",
        "    vector_dimension = 200,\n",
        ")\n",
        "\n",
        "original_embedding_matrix = embedding.create_embedding_matrix(original_tokenizer, largest_vocab_size)\n",
        "clone_embedding_matrix = embedding.create_embedding_matrix(clone_tokenizer, largest_vocab_size)\n",
        "original_embedding_dim = 200\n",
        "clone_embedding_dim = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmUuUvwmDPFx",
        "outputId": "365445e9-a289-463b-d6dc-689fd6a877ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Get index to word from word to index dict\n",
        "def inverse_mapping(f):\n",
        "    return f.__class__(map(reversed, f.items()))\n",
        "\n",
        "# Build encoding dictionary\n",
        "original_encoding, original_decoding = dict(original_tokenizer.word_index), inverse_mapping(dict(original_tokenizer.word_index))\n",
        "clone_encoding, clone_decoding = dict(clone_tokenizer.word_index), inverse_mapping(dict(clone_tokenizer.word_index))\n",
        "\n",
        "# Transform the data\n",
        "encoded_training_input = original_tokenizer.texts_to_sequences(training_input)\n",
        "encoded_training_output = clone_tokenizer.texts_to_sequences(training_output)\n",
        "\n",
        "\n",
        "# Encoder Input\n",
        "training_encoder_input = pad_sequences(encoded_training_input, maxlen=DEFAULT_INPUT_LENGTH)\n",
        "# Decoder Input (need padding by START_CHAR_CODE)\n",
        "# training_decoder_input = np.zeros_like(encoded_training_output)\n",
        "# training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
        "# training_decoder_input[:, 0] = START_CHAR_CODE\n",
        "training_decoder_input = pad_sequences(encoded_training_input, maxlen=DEFAULT_INPUT_LENGTH)\n",
        "encoded_training_output = pad_sequences(encoded_training_output, maxlen=DEFAULT_INPUT_LENGTH)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyMkTS5uhC7_"
      },
      "source": [
        "# One hot encoding in chunks\n",
        "\n",
        "size = int(len(encoded_training_output) / 30)\n",
        "for i in range(1, 35):\n",
        "    with gzip.open(f'pickle/{i}.pkl', 'wb') as f:\n",
        "        array = np.eye(clone_vocab_size+1)[encoded_training_output[size*(i-1):size*i]]\n",
        "        pickle.dump(array, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGdSi3Mr2ZBj"
      },
      "source": [
        "def output_gen():        \n",
        "    for i in range(1, 31):\n",
        "        with gzip.open(f'pickle/{i}.pkl', 'rb') as f:\n",
        "            yield pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCjFw-QzD5P-",
        "outputId": "9316dbc0-4f1b-4f73-9980-10f24f289bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(size)\n",
        "def model_generator():\n",
        "    while True:\n",
        "        for index, output in enumerate(output_gen()):\n",
        "            for i in range(size):\n",
        "                yield ([np.expand_dims(training_encoder_input[size*index+i], axis=0), np.expand_dims(training_decoder_input[size*index+i], axis=0)], np.expand_dims(output[i], axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7kE5dDPDPLc",
        "outputId": "ded76543-0fc8-40f7-a6af-3c1444949a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "def create_model(\n",
        "        input_length=20,\n",
        "        output_length=20):\n",
        "\n",
        "    encoder_input = tf.keras.Input(shape=(input_length,))\n",
        "    decoder_input = tf.keras.Input(shape=(output_length,))\n",
        "\n",
        "    encoder = tf.keras.layers.Embedding(original_embedding_matrix.shape[0], original_embedding_dim, weights=[original_embedding_matrix], trainable=False)(encoder_input)\n",
        "    encoder, h_encoder, u_encoder = tf.keras.layers.LSTM(64, return_state=True)(encoder)\n",
        "\n",
        "    decoder = tf.keras.layers.Embedding(clone_embedding_matrix.shape[0], clone_embedding_dim, weights=[clone_embedding_matrix], trainable=False)(decoder_input)\n",
        "    decoder = tf.keras.layers.LSTM(64, return_sequences=True)(decoder, initial_state=[h_encoder, u_encoder])\n",
        "    decoder = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(clone_vocab_size+1))(decoder)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=[decoder])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(model_generator(), \n",
        "          epochs=20, \n",
        "          batch_size=4, \n",
        "          steps_per_epoch=training_decoder_input.shape[0]//256,\n",
        "          verbose=1)\n",
        "\n",
        "model.save('model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 20, 200)      4400200     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20, 200)      4400200     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 64), (None,  67840       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 64)       67840       embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 20, 22001)    1430065     lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 10,366,145\n",
            "Trainable params: 1,565,745\n",
            "Non-trainable params: 8,800,400\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 7.6392e-05 - accuracy: 0.1662\n",
            "Epoch 2/20\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 5.3026e-05 - accuracy: 0.1974\n",
            "Epoch 3/20\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 4.8830e-05 - accuracy: 0.2064\n",
            "Epoch 4/20\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 4.5212e-05 - accuracy: 0.2073\n",
            "Epoch 5/20\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 4.2382e-05 - accuracy: 0.2432\n",
            "Epoch 6/20\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 4.2661e-05 - accuracy: 0.2175\n",
            "Epoch 7/20\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 4.2452e-05 - accuracy: 0.2000\n",
            "Epoch 8/20\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 4.1057e-05 - accuracy: 0.2175\n",
            "Epoch 9/20\n",
            " 59/117 [==============>...............] - ETA: 0s - loss: 3.9572e-05 - accuracy: 0.2432"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}